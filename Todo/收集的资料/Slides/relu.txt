1.ReLU非线性特征 
一句话概括：不用simgoid和tanh作为激活函数，而用ReLU作为激活函数的原因是：加速收敛。
因为sigmoid和tanh都是饱和(saturating)的。何为饱和？个人理解是把这两者的函数曲线和导数曲线plot出来就知道了：他们的导数都是倒过来的碗状，也就是，越接近目标，对应的导数越小。而ReLu的导数对于大于0的部分恒为1。于是ReLU确实可以在BP的时候能够将梯度很好地传到较前面的网络。
ReLU（线性纠正函数）取代sigmoid函数去激活神经元